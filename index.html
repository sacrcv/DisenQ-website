<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DisenQ: Disentangling Q-Former for Activity-Biometrics">
 
  <meta name="keywords" content="Q-Former, feature disentanglement, person re-identification, activity-aware">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>DisenQ: Disentangling Q-Former for Activity-Biometrics</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>


<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-1 publication-title">DisenQ: Disentangling Q-Former for Activity-Biometrics</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- put shhereen link -->
                <a href="https://sacrcv.github.io" target="_blank">Shehreen Azad,</a></span>
                <span class="author-block">
                    <a href="https://www.crcv.ucf.edu/person/rawat/" target="_blank">Yogesh Singh Rawat</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Center for Research in Computer Vision, University of Central Florida.
                  <span class="author-block">
                       <br> <h1 class="title is-4"> </h1>
                  </span>
                    </div>

                  <div class="is-size-5 publication-authors">
                  <h1 class="title is-4">
              <font color="#B03A2E"><b>ICCV 2025 Highlight ⭐️</b></font>
            </h1>
          </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                      <!-- Paper--> 
                       <span class="link-block">
                        <a href="https://arxiv.org/pdf/2507.07262" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> 
                  
                    <!-- Arxiv abstract-->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2507.07262" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                </span>

                  <!-- Github-->
                  <span class="link-block">
                    <a href="https://github.com/sacrcv/DisenQ.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Poster -->
                <span class="link-block">
                  <a href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/187.png?t=1758636726.6472878" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-image"></i></span>
                    <span>Poster</span>
                  </a>
                </span>


              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">


      <div style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px;">
        <img src="static/images/gait_2.gif" style="height:140px;">
        <img src="static/images/gait_3.gif" style="height:140px;">

        <img src="static/images/gait_to_activities.png" style="height:80px;" >

        <img src="static/images/activity_2.gif" style="height:140px;">
        <img src="static/images/activity_3.gif" style="height:140px;">
        <img src="static/images/activity_4.gif" style="height:140px;">
        </div>
      <h2 class="subtitle has-text-centered">
     Scaling person identification beyond walking to diverse real-world activities; enabling practical recognition in surveillance, healthcare and many more etc.
      </h2>

    </div>
  </div>
</section>

 <!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we address activity-biometrics, which involves identifying individuals across diverse set of activities. Unlike traditional person identification, this setting introduces additional challenges as identity cues become entangled with motion dynamics and appearance variations, making biometrics feature learning more complex. While additional visual data like pose and/or silhouette help, they often struggle from extraction inaccuracies. To overcome this, we propose a multimodal language-guided framework that replaces reliance on additional visual data with structured textual supervision. At its core, we introduce <b>DisenQ</b> (<b>Disen</b>tangling <b>Q</b>-Former), a unified querying transformer that disentangles biometrics, motion, and non-biometrics features by leveraging structured language guidance. This ensures identity cues remain independent of appearance and motion variations, preventing misidentifications. We evaluate our approach on three activity-based video benchmarks, achieving state-of-the-art performance. Additionally, we demonstrate strong generalization to complex real-world scenario with competitive performance on a traditional video-based identification benchmark, showing the effectiveness of our framework.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract --> 


<!-- Challenges and Solution -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Current Challenges and Our Solution</h2>
      </div>
    </div>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       
        <div class="item has-text-centered"> 
        <!-- Your image here -->
        <img src="static/images/traditional_pipeline.png"  style="height:280px;"/>
        <h2 class="subtitle has-text-centered">
          “Traditional multimodal models use RGB images with extra visual inputs like silhouettes or poses, which works for gait but fails for diverse activities. Using text for alignment alone isn't enough.”
        </h2>
      </div>


        <div class="item has-text-centered"> 
        <!-- Your image here -->
        <img src="static/images/probe.gif"  style="height:280px; margin-right:60px;"/>
        <img src="static/images/challenge_1.png" style="height: 280px;"/>
        <h2 class="subtitle has-text-centered">
          Current multimodal person identification models depend heavily on clean silhouettes or poses, which fail in noisy real-world activities and cause identity mismatches. We address this by replacing such dependencies with structured textual descriptions of biometrics, motion, and non-biometrics, enabling both alignment and explicit disentanglement of the visual space.
        </h2>
      </div>

      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/probe.gif"  style="height:280px; margin-right:60px"/>
        <img src="static/images/appearance_1.gif"  style="height:280px; margin-right:10px;"/>
        <img src="static/images/appearance_2.gif"  style="height:280px; margin-right:60px;"/>
        <img src="static/images/appearance_3.gif"  style="height:280px; margin-right:10px;"/>
        <img src="static/images/appearance_4.gif"  style="height:280px;"/>
        <h2 class="subtitle has-text-centered">
          Disentanglement is crucial because existing multimodal models often conflate identity with appearance cues like clothing color, leading to mismatches (1st pair). We explicit separate biometrics features from appearance through textual guidance, and thus making identity recognition robust to such variations (2nd pair).
        </h2>
      </div>
      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/probe.gif"  style="height:280px; margin-right:60px;"/>
        <img src="static/images/motion_1.gif"  style="height:280px; margin-right:10px;"/>
        <img src="static/images/motion_2.gif"  style="height:280px; margin-right:60px;"/>
        <img src="static/images/motion_3.gif"  style="height:280px; margin-right:10px;"/>
        <img src="static/images/motion_4.gif"  style="height:280px;"/>
        <h2 class="subtitle has-text-centered">
         Similarly, current models also confuse people performing the same activity (1st pair) due to not having the explicit knowledge of distinct actions and thus over-relying on motion pattern. By disentangling motion, we ensure only identity-specific relevant actions guide recognition (2nd pair).
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- method -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       
        <div class="item has-text-centered"> 
        <!-- Your image here -->
        <img src="static/images/pipeline.png"  style="height:280px;"/>
        <h2 class="subtitle has-text-centered">
          Our framework takes a video and three text prompts (biometrics, motion, non-biometrics) to disentangle visual features into separate spaces with <b>DisenQ</b>. Identity recognition is then guided by biometrics and relevant motion features, weighted by their relevance through the identification head.
        </h2>
      </div>

      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/disenq.png" style="height:400px;"/>
        <h2 class="subtitle has-text-centered">
          DisenQ replaces the single-query traditional Q-Former with three learnable query sets: biometrics, motion, and non-biometrics. Each refines features via self- and cross-attention with visual and text inputs, producing disentangled spaces. 
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- quantitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Quantitative Analysis</h2>
      </div>
    </div>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       
        <div class="item has-text-centered"> 
        <!-- Your image here -->
        <img src="static/images/quant_1.png" style="height: 400px;" />
        <h2 class="subtitle has-text-centered">
          State-of-the-art performance on activity-biometrics benchmarks: NTU RGB-AB, PKU MMD-AB, Charades-AB.
        </h2>
      </div>

      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/quant_2.png" style="height: 400px;"/>
        <h2 class="subtitle has-text-centered">
          Strong generalization ability on traditional video person re-identification dataset MEVID.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- ablation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h2 class="title is-3">Ablation Studies</h2>
    </div> </div>
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered has-text-justified"></div>
      <img src="./static/images/abla_1.png" style="height: 400px;"/> 
      <h2 class="subtitle has-text-centered">
        Ablation shows step-wise gain, solving each of the challenges one-by-one.
      </h2>
      </div>
    </div>
  </div>
  </section>

  <!-- Qualitative analysis -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Qualitative Analysis</h2>
      </div>
    </div>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       
        <div class="item has-text-centered"> 
        <!-- Your image here -->
        <img src="static/images/tsne.png" style="height: 280px;" />
        <h2 class="subtitle has-text-centered">
          DisenQ transforms the overlapped baseline feature space into distinct clusters for identity, appearance, and motion, enabling robust recognition.
        </h2>
      </div>

      <div class="item has-text-centered">
        <!-- Your image here -->
        <img src="static/images/quant_3.png" style="height:280px; margin-right:100px;"/>
        <img src="static/images/probe_2.gif" style="height:280px; margin-right:60px;"/>
        <img src="static/images/cross_act_1.gif" style="height:280px; margin-right:10px;"/>
        <img src="static/images/cross_act_2.gif" style="height:280px;"/>
        <h2 class="subtitle has-text-centered">
          Superior generalization across-activities. 
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!--BibTex citation -->
  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{azad2025disenq,
  title={Disenq: Disentangling q-former for activity-biometrics},
  author={Azad, Shehreen and Rawat, Yogesh S},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2025}}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://nerfies.github.io" target="_blank">Nerfies project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>


  </body>
  </html>
